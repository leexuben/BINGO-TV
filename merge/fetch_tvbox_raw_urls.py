import os
import requests
import re
import subprocess

# é…ç½®éƒ¨åˆ†
KEYWORDS = ['èç‰‡', 'é‡‡é›†', '.spider']
OUTPUT_DIR = 'merge'
OUTPUT_FILE = 'tvbox_raw_sources.txt'
TOKEN_ENV_NAME = 'MY_GH_TOKEN'  # ç”¨äºGitHub APIè®¤è¯çš„Tokenç¯å¢ƒå˜é‡å

# ä»ç¯å¢ƒå˜é‡è·å–GitHub Token
gh_token = os.getenv(TOKEN_ENV_NAME)
headers = {}
if gh_token:
    headers = {
        'Authorization': f'token {gh_token}',
        'Accept': 'application/vnd.github.v3+json'
    }

# ç”¨äºåŒ¹é…Raw URLçš„æ­£åˆ™è¡¨è¾¾å¼
RAW_URL_REGEX = re.compile(r'https://raw\.githubusercontent\.com/[\w\-]+/[\w\-]+/[\w\-]+/[\w\-/]+')

def extract_raw_urls_from_content(content):
    """ä»æ–‡ä»¶å†…å®¹ä¸­æå–Raw URL"""
    return RAW_URL_REGEX.findall(content)

def main():
    all_raw_urls = []

    for keyword in KEYWORDS:
        print(f"ğŸ” æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}")
        search_url = f'https://api.github.com/search/code?q={keyword}+in:file&per_page=100'
        try:
            response = requests.get(search_url, headers=headers)
            response.raise_for_status()
            data = response.json()
            items = data.get('items', [])
            print(f"âœ… æ‰¾åˆ° {len(items)} ä¸ªåŒ…å« '{keyword}' çš„æ–‡ä»¶")

            for item in items:
                file_html_url = item.get('html_url')
                try:
                    # è·å–æ–‡ä»¶å†…å®¹
                    file_content_url = item.get('download_url')
                    if not file_content_url:
                        print(f"âš ï¸ æ–‡ä»¶ {file_html_url} æ— å†…å®¹é“¾æ¥ï¼Œè·³è¿‡")
                        continue
                    content_response = requests.get(file_content_url, headers=headers)
                    content_response.raise_for_status()
                    file_content = content_response.text
                    # æå–Raw URL
                    raw_urls = extract_raw_urls_from_content(file_content)
                    if raw_urls:
                        print(f"ğŸ”— ä»æ–‡ä»¶ {file_html_url} ä¸­æå–åˆ° {len(raw_urls)} ä¸ªRaw URL")
                        all_raw_urls.extend(raw_urls)
                    else:
                        print(f"âš ï¸ æ–‡ä»¶ {file_html_url} ä¸­æœªæå–åˆ°Raw URL")
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {file_html_url} æ—¶å‡ºé”™: {e}")

        except requests.exceptions.RequestException as e:
            print(f"âŒ æœç´¢å…³é”®è¯ '{keyword}' æ—¶å‘ç”Ÿè¯·æ±‚é”™è¯¯: {e}")

    # å»é‡
    unique_raw_urls = list(set(all_raw_urls))
    print(f"ğŸ”¢ æ€»å…±æ‰¾åˆ° {len(unique_raw_urls)} ä¸ªå”¯ä¸€Raw URL")

    # ä¿å­˜åˆ°æ–‡ä»¶
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
    if unique_raw_urls:
        with open(output_path, 'w', encoding='utf-8') as f:
            for url in unique_raw_urls:
                f.write(url + '
')
        print(f"âœ… Raw URLå·²æˆåŠŸä¿å­˜åˆ°æ–‡ä»¶: {output_path}")

        # Gitæ“ä½œ
        try:
            subprocess.run(['git', 'add', output_path], check=True)
            subprocess.run([
                'git', 'commit',
                '-m', 'ğŸ¤– è‡ªåŠ¨æ›´æ–°TVBoxå…¨ç½‘Rawæºæ–‡ä»¶é“¾æ¥ (å…³é”®è¯ï¼šèç‰‡ã€é‡‡é›†ã€.spider)'
            ], check=True)
            subprocess.run(['git', 'push'], check=True)
            print("âœ… Gitæäº¤å¹¶æ¨é€æˆåŠŸ")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Gitæ“ä½œå¤±è´¥: {e}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„Raw URLï¼Œä¸è¿›è¡ŒGitæäº¤")

if __name__ == '__main__':
    main()

