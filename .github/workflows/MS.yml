on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'

jobs:
  merge-sites:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout target repository
      uses: actions/checkout@v4
      with:
        repository: leexuben/BINGO-TV
        path: bingo-tv
        token: ${{ secrets.GH_TOKEN }}
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install requests

    - name: Run merge script with JAR integration
      run: |
        python << 'EOF'
        import json
        import requests
        import re
        import os
        from datetime import datetime

        # æºæ–‡ä»¶é…ç½®ï¼šæ¯ä¸ªAPIæ–‡ä»¶å¯¹åº”ä¸€ä¸ªJARæ–‡ä»¶
        SOURCE_CONFIGS = [
            {
                "api_url": "https://raw.githubusercontent.com/leexuben/TV-BOX/refs/heads/main/tvbox/ç‹äºŒå°/api.json",
                "jar_url": "https://raw.githubusercontent.com/leexuben/TV-BOX/refs/heads/main/tvbox/ç‹äºŒå°/spider.jar",
                "source": "ç‹äºŒå°"
            },
            {
                "api_url": "https://raw.githubusercontent.com/leexuben/TV-BOX/refs/heads/main/tvbox/æ½‡æ´’/api.json",
                "jar_url": "https://raw.githubusercontent.com/leexuben/TV-BOX/refs/heads/main/tvbox/æ½‡æ´’/spider.jar",
                "source": "æ½‡æ´’"
            },
            {
                "api_url": "https://raw.githubusercontent.com/leexuben/TV-BOX/refs/heads/main/tvbox/å·§æŠ€/api.json",
                "jar_url": "https://raw.githubusercontent.com/leexuben/TV-BOX/refs/heads/main/tvbox/å·§æŠ€/spider.jar",
                "source": "å·§æŠ€"
            },
            {
                "api_url": "https://raw.githubusercontent.com/leexuben/ljlfct01.github.io/refs/heads/main/å½±è§†",
                "jar_url": "https://raw.githubusercontent.com/leexuben/ljlfct01.github.io/refs/heads/main/jar/spider.jar",
                "source": "å½±è§†"
            }
        ]
        
        # ç›®æ ‡æ–‡ä»¶è·¯å¾„
        TARGET_FILE = "bingo-tv/ç»¼åˆå½±è§†.json"

        def extract_json_from_xml(content):
            """ä»XMLå†…å®¹ä¸­æå–JSONæ•°æ®"""
            try:
                json_start = content.find('{')
                json_end = content.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    json_content = content[json_start:json_end]
                    return json.loads(json_content)
            except:
                pass
            return None

        def extract_sites_with_regex(content):
            """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–sitesæ•°ç»„"""
            try:
                pattern = r'"sites"\s*:\s*\[(.*?)\]'
                match = re.search(pattern, content, re.DOTALL)
                if match:
                    sites_content = match.group(1)
                    full_array = f'[{sites_content}]'
                    return json.loads(full_array)
            except:
                pass
            return []

        def extract_sites_from_content(content):
            """ä»å†…å®¹ä¸­æå–sitesæ•°æ®"""
            # æ–¹æ³•1: ç›´æ¥è§£æJSON
            try:
                data = json.loads(content)
                sites = data.get('sites', [])
                if sites:
                    return sites
            except:
                pass
            
            # æ–¹æ³•2: å¤„ç†XMLåŒ…è£…çš„JSON
            xml_data = extract_json_from_xml(content)
            if xml_data:
                sites = xml_data.get('sites', [])
                if sites:
                    return sites
            
            # æ–¹æ³•3: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–
            sites = extract_sites_with_regex(content)
            if sites:
                return sites
            
            return []

        def fetch_url_content(url):
            """è·å–URLå†…å®¹"""
            try:
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                return response.text
            except Exception as e:
                print(f"   âŒ è·å–å¤±è´¥: {e}")
                return None

        def add_jar_to_site(site, jar_url, source_name):
            """åªä¸ºæ²¡æœ‰jarå­—æ®µçš„ç«™ç‚¹æ·»åŠ jarä¿¡æ¯"""
            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰jarå­—æ®µ
            if 'jar' in site and site['jar']:
                return site, False  # è¿”å›Falseè¡¨ç¤ºæœªæ·»åŠ 
            
            # æ£€æŸ¥æ˜¯å¦æœ‰jarå­—æ®µä½†ä¸ºç©º
            if 'jar' in site and not site['jar']:
                site['jar'] = jar_url
                site['source'] = source_name
                return site, True
            
            # æ²¡æœ‰jarå­—æ®µï¼Œæ·»åŠ jarä¿¡æ¯
            site['jar'] = jar_url
            site['source'] = source_name
            
            return site, True

        def main():
            print("ğŸš€ å¼€å§‹åˆå¹¶ç«™ç‚¹æ•°æ®ï¼ˆèåˆJARæ·»åŠ åŠŸèƒ½ï¼‰...")
            print("=" * 60)
            
            all_sites = []
            processed_files = []
            jar_added_count = 0
            jar_existing_count = 0
            
            for config in SOURCE_CONFIGS:
                api_url = config["api_url"]
                jar_url = config["jar_url"]
                source_name = config["source"]
                
                print(f"ğŸ“¥ å¤„ç†: {source_name}")
                
                content = fetch_url_content(api_url)
                if content:
                    sites = extract_sites_from_content(content)
                    if sites:
                        source_jar_added = 0
                        source_jar_existing = 0
                        
                        modified_sites = []
                        for site in sites:
                            # åªä¸ºæ²¡æœ‰jarçš„ç«™ç‚¹æ·»åŠ jarä¿¡æ¯
                            modified_site, jar_added = add_jar_to_site(site, jar_url, source_name)
                            modified_sites.append(modified_site)
                            
                            if jar_added:
                                source_jar_added += 1
                                jar_added_count += 1
                            else:
                                source_jar_existing += 1
                                jar_existing_count += 1
                        
                        all_sites.extend(modified_sites)
                        processed_files.append({
                            "source": source_name,
                            "sites_count": len(sites),
                            "jar_added": source_jar_added,
                            "jar_existing": source_jar_existing
                        })
                        print(f"   âœ… æå–åˆ° {len(sites)} ä¸ªç«™ç‚¹")
                        if source_jar_added > 0:
                            print(f"       æ–°å¢JAR: {source_jar_added} ä¸ª")
                        if source_jar_existing > 0:
                            print(f"       å·²æœ‰JAR: {source_jar_existing} ä¸ª")
                    else:
                        print(f"   âš ï¸  æœªæ‰¾åˆ°sitesæ•°æ®")
                else:
                    print(f"   âŒ æ— æ³•è·å–æ–‡ä»¶å†…å®¹")
                print("-" * 40)
            
            if not all_sites:
                print("âŒ æœªæå–åˆ°ä»»ä½•ç«™ç‚¹æ•°æ®")
                return
            
            # å»é‡å¤„ç†ï¼ˆåŸºäºkeyå­—æ®µï¼‰
            unique_sites = []
            seen_keys = set()
            duplicate_count = 0
            
            for site in all_sites:
                key = site.get('key', '')
                if key and key in seen_keys:
                    duplicate_count += 1
                    continue
                
                unique_sites.append(site)
                if key:
                    seen_keys.add(key)
            
            print("ğŸ“Š åˆå¹¶ç»Ÿè®¡:")
            print(f"   æ€»ç«™ç‚¹æ•°: {len(all_sites)}")
            print(f"   å»é‡åç«™ç‚¹æ•°: {len(unique_sites)}")
            print(f"   é‡å¤ç«™ç‚¹æ•°: {duplicate_count}")
            if jar_added_count > 0:
                print(f"   æ–°å¢JARç«™ç‚¹æ•°: {jar_added_count}")
            if jar_existing_count > 0:
                print(f"   å·²æœ‰JARç«™ç‚¹æ•°: {jar_existing_count}")
            
            # åˆ›å»ºåˆå¹¶ç»“æœ
            result_data = {
                "sites": unique_sites,
                "metadata": {
                    "description": "è‡ªåŠ¨åˆå¹¶çš„å½±è§†ç«™ç‚¹é›†åˆ",
                    "total_sites": len(unique_sites),
                    "jar_added": jar_added_count,
                    "jar_existing": jar_existing_count,
                    "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
            }
            
            # ä¿å­˜åˆ°ç›®æ ‡æ–‡ä»¶
            with open(TARGET_FILE, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            print("âœ… åˆå¹¶å®Œæˆ! ç«™ç‚¹æ•°æ®å·²èåˆJARä¿¡æ¯")

        if __name__ == "__main__":
            main()
        EOF

    - name: Configure Git and commit changes
      env:
        GH_TOKEN: ${{ secrets.GH_TOKEN }}
      run: |
        cd bingo-tv
        
        # é…ç½®Gitç”¨æˆ·ä¿¡æ¯
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        
        # é…ç½®è®¤è¯
        git remote set-url origin https://x-access-token:${GH_TOKEN}@github.com/leexuben/BINGO-TV.git
        
        # æ·»åŠ æ‰€æœ‰æ›´æ”¹
        git add .
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ”¹éœ€è¦æäº¤
        if git diff --cached --quiet; then
            echo "ğŸ“ æ²¡æœ‰æ›´æ”¹éœ€è¦æäº¤"
            exit 0
        fi
        
        # æäº¤æ›´æ”¹
        git commit -m "Auto-update: Merged sites data with JAR integration $(date +'%Y-%m-%d %H:%M:%S')"
        
        # æ¨é€æ›´æ”¹
        git push origin main
        
        echo "âœ… æ›´æ”¹å·²æˆåŠŸæäº¤å¹¶æ¨é€"
